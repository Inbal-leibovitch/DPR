# Use Dense Passage Retrieval for Implicit reasoning on StrategyQA dataset

In this work we  performed context retrieval using DPR model for StrategyQA dataset.

## StrategyQA
StrategyQA dataset consists of 2,780 implicit reasoning questions. Each includes a questions, its decomposition to steps, and evidence paragraphs.
The official code can be found [here](https://github.com/eladsegal/strategyqa). 
The dataset dataset is available [here](https://allenai.org/data/strategyqa)  

## DPR
The context retieval model we used is based on [Dense Passage Retrieval](https://github.com/facebookresearch/DPR) (`DPR`) - a set of tools and models for state-of-the-art open-domain Q&A research.
It is based on the following paper:

Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih, [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906), Preprint 2020.

### Features
1. Dense retriever model is based on bi-encoder architecture.
2. Extractive Q&A reader&ranker joint model inspired by [this](https://arxiv.org/abs/1911.03868) paper.
3. Related data pre- and post- processing tools.
4. Dense retriever component for inference time logic is based on FAISS index.


## Installation

Installation from the source. Python's virtual or Conda environments are recommended.

```bash
git clone git@github.com:Inbal-leibovitch/DPR.git
cd DPR
pip install .
```

## Fine tuning the DPR model on strategyQA dataset

### Retriever input data format
The default data format of the Retriever training data is JSON.
It contains pools of 2 types of negative passages per question, as well as positive passages and some additional information.

```
[
  {
	"question": "....",
	"answers": ["...", "...", "..."],
	"positive_ctxs": [{
		"title": "...",
		"text": "...."
	}],
	"negative_ctxs": ["..."],
	"hard_negative_ctxs": ["..."]
  },
  ...
]
```
### Data preprocessing
First, we converted the questions in the dataset to the DPR required format.
The converted train and dev sets are under ```conf/datasets/strategyQA_{dev/train}.json```


### Run fine tuning 
```
python train_dense_encoder.py train_datasets=[strategy_train] dev_datasets=[strategy_dev] output_dir=output_path 
```

We fine-tuned the model for 30 epochs. The [fine tuned model](https://drive.google.com/file/d/1sQP2IApNxJafezdeLI9L4tvw1IaI_gYf/view?usp=sharing) is available. 


## DPR retriever inference on StrategyQA dataset
We used DPR for inference on StrategyQA datset. During inference, the model retrieves the most relevant paragraphs to the question from the corpus. 

### Convert strategyQA corpus to DPR format
We converted the corpus used by strategyQA to the DPR format so that it could be loaded and indexed by the model during inference. 

To convert the corpus:
```paragraph_file``` should be the strategyQA corpus file. Specify the name of the converted corpus in ```out-file```.
```
conversions/convert_paragraphs_file_to_DPR_ctx_file --paragraphs_file corpus.json --out-file corput.tsv
``` 
 
### Generate dense embeddings
Generating representation vectors for the static documents dataset is a highly parallelizable process which can take up to a few days if computed on a single GPU. You might want to use multiple available GPU servers by running the script on each of them independently and specifying their own shards.
```
 python generate_dense_embeddings.py --model_file {path to biencoder checkpoint} --out_file {result file location} --ctx_file {path to corpus file} --shard_id {shard_num, 0-based} --num_shards {total num of shards}
 ```

### Run Inference
We evaluated the inference of both the DPR bert-base model, and the model that was fine tuned on StrategyQA dataset.
```
python dense_retriever.py --model_file {path to model checkpoint} --ctx_file {path to corpus tsv file} --qa_file {path to questions file} --encoded_ctx_file {encoded document files glob expression} --out_file {path to retriever resutls} --n-docs 200
```

The tool writes retrieved results for subsequent reader model training into specified out_file.
It is a json with the following format:

```
[
    {
        "question": "...",
        "answers": ["...", "...", ... ],
        "ctxs": [
            {
                "id": "...", # passage id from database tsv file
                "title": "",
                "text": "....",
                "score": "...",  # retriever score
                "has_answer": true|false
     },
]
```

Results are sorted by their similarity score, from most relevant to least relevant.

By default, dense_retriever uses exhaustive search process, but you can opt in to use lossy index types.
We provide HNSW and HNSW_SQ index options.
Enabled them by indexer=hnsw or indexer=hnsw_sq command line arguments.


## 4. Evaluate retrieval accuracy for strategyQA dataset 
We evaluated the models performance both on the dev and test sets. We used the evaluation code provided by Strategy QA. 
To convert the retriever results to StrategyQA evlauator format run:
```
conversions/convert_retriever_results_to_evaluator_format.py --retriever_output_file {path to retriever result file} --out_file {path to converted result}
```

### Dev set evaluation 
To evaluate against the dev set we used the StrategyQA evaluator avilable here

Our results on dev set: Recall@10 = 0.13864628820960698

### Test set evaluation
To evaluate against the test set we submitted the results of both our models (the base model and the fine tuned one) to the StrategyQA leaderboard.

The resutls are:
base model: Recall@10 = 0.1254
fine-tuned model: Recall@10 = 0.1206



